{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import shutil\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "from dataloader import GetDocumentObj, DictObj, GetTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "GPU_ID = 2\n",
    "\n",
    "USE_CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_path = \"top_B.parse\"\n",
    "u20_path = \"u20_B.parse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2274"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dict = {top_path:0, u20_path:1}\n",
    "list_document = []; start_date = None ; end_date = None\n",
    "list_document, start_date, end_date = GetDocumentObj(list_document, top_path, label_dict[top_path], start_date, end_date)\n",
    "list_document, start_date, end_date = GetDocumentObj(list_document, u20_path, label_dict[u20_path], start_date, end_date)\n",
    "\n",
    "documents_obj = DictObj()\n",
    "documents_obj.SetDict(list_document)    \n",
    "time_tensor, loc_tensor, person_organ_tensor, morph_tensor, list_label = GetTensor(list_document, documents_obj, start_date, end_date)\n",
    "\n",
    "len(list_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_tensor2, _, _, _, _ = GetTensor(list_document, documents_obj, start_date, end_date, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\n",
       "Columns 0 to 9 \n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "\n",
       "Columns 10 to 19 \n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "\n",
       "Columns 20 to 29 \n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "\n",
       "Columns 30 to 39 \n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "\n",
       "Columns 40 to 49 \n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "\n",
       "Columns 50 to 59 \n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "\n",
       "Columns 60 to 69 \n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "\n",
       "Columns 70 to 79 \n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "\n",
       "Columns 80 to 89 \n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "\n",
       "Columns 90 to 99 \n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "\n",
       "Columns 100 to 109 \n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0001  0.0001  0.0001  0.0002\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0001  0.0001  0.0001  0.0002\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0001  0.0001  0.0001  0.0002\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0001  0.0001  0.0001  0.0002\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0001  0.0001  0.0001  0.0002\n",
       "\n",
       "Columns 110 to 119 \n",
       " 0.0003  0.0005  0.0007  0.0011  0.0015  0.0022  0.0031  0.0043  0.0060  0.0082\n",
       " 0.0003  0.0005  0.0007  0.0011  0.0015  0.0022  0.0031  0.0043  0.0060  0.0082\n",
       " 0.0003  0.0005  0.0007  0.0011  0.0015  0.0022  0.0031  0.0043  0.0060  0.0082\n",
       " 0.0003  0.0005  0.0007  0.0011  0.0015  0.0022  0.0031  0.0043  0.0060  0.0082\n",
       " 0.0003  0.0005  0.0007  0.0011  0.0015  0.0022  0.0031  0.0043  0.0060  0.0082\n",
       "\n",
       "Columns 120 to 129 \n",
       " 0.0111  0.0149  0.0198  0.0261  0.0340  0.0439  0.0561  0.0710  0.0889  0.1103\n",
       " 0.0111  0.0149  0.0198  0.0261  0.0340  0.0439  0.0561  0.0710  0.0889  0.1103\n",
       " 0.0111  0.0149  0.0198  0.0261  0.0340  0.0439  0.0561  0.0710  0.0889  0.1103\n",
       " 0.0111  0.0149  0.0198  0.0261  0.0340  0.0439  0.0561  0.0710  0.0889  0.1103\n",
       " 0.0111  0.0149  0.0198  0.0261  0.0340  0.0439  0.0561  0.0710  0.0889  0.1103\n",
       "\n",
       "Columns 130 to 139 \n",
       " 0.1353  0.1645  0.1979  0.2357  0.2780  0.3247  0.3753  0.4296  0.4868  0.5461\n",
       " 0.1353  0.1645  0.1979  0.2357  0.2780  0.3247  0.3753  0.4296  0.4868  0.5461\n",
       " 0.1353  0.1645  0.1979  0.2357  0.2780  0.3247  0.3753  0.4296  0.4868  0.5461\n",
       " 0.1353  0.1645  0.1979  0.2357  0.2780  0.3247  0.3753  0.4296  0.4868  0.5461\n",
       " 0.1353  0.1645  0.1979  0.2357  0.2780  0.3247  0.3753  0.4296  0.4868  0.5461\n",
       "\n",
       "Columns 140 to 149 \n",
       " 0.6065  0.6670  0.7261  0.7827  0.8353  0.8825  0.9231  0.9560  0.9802  0.9950\n",
       " 0.6065  0.6670  0.7261  0.7827  0.8353  0.8825  0.9231  0.9560  0.9802  0.9950\n",
       " 0.6065  0.6670  0.7261  0.7827  0.8353  0.8825  0.9231  0.9560  0.9802  0.9950\n",
       " 0.6065  0.6670  0.7261  0.7827  0.8353  0.8825  0.9231  0.9560  0.9802  0.9950\n",
       " 0.6065  0.6670  0.7261  0.7827  0.8353  0.8825  0.9231  0.9560  0.9802  0.9950\n",
       "\n",
       "Columns 150 to 159 \n",
       " 1.0000  0.9950  0.9802  0.9560  0.9231  0.8825  0.8353  0.7827  0.7261  0.6670\n",
       " 1.0000  0.9950  0.9802  0.9560  0.9231  0.8825  0.8353  0.7827  0.7261  0.6670\n",
       " 1.0000  0.9950  0.9802  0.9560  0.9231  0.8825  0.8353  0.7827  0.7261  0.6670\n",
       " 1.0000  0.9950  0.9802  0.9560  0.9231  0.8825  0.8353  0.7827  0.7261  0.6670\n",
       " 1.0000  0.9950  0.9802  0.9560  0.9231  0.8825  0.8353  0.7827  0.7261  0.6670\n",
       "\n",
       "Columns 160 to 169 \n",
       " 0.6065  0.5461  0.4868  0.4296  0.3753  0.3247  0.2780  0.2357  0.1979  0.1645\n",
       " 0.6065  0.5461  0.4868  0.4296  0.3753  0.3247  0.2780  0.2357  0.1979  0.1645\n",
       " 0.6065  0.5461  0.4868  0.4296  0.3753  0.3247  0.2780  0.2357  0.1979  0.1645\n",
       " 0.6065  0.5461  0.4868  0.4296  0.3753  0.3247  0.2780  0.2357  0.1979  0.1645\n",
       " 0.6065  0.5461  0.4868  0.4296  0.3753  0.3247  0.2780  0.2357  0.1979  0.1645\n",
       "\n",
       "Columns 170 to 179 \n",
       " 0.1353  0.1103  0.0889  0.0710  0.0561  0.0439  0.0340  0.0261  0.0198  0.0149\n",
       " 0.1353  0.1103  0.0889  0.0710  0.0561  0.0439  0.0340  0.0261  0.0198  0.0149\n",
       " 0.1353  0.1103  0.0889  0.0710  0.0561  0.0439  0.0340  0.0261  0.0198  0.0149\n",
       " 0.1353  0.1103  0.0889  0.0710  0.0561  0.0439  0.0340  0.0261  0.0198  0.0149\n",
       " 0.1353  0.1103  0.0889  0.0710  0.0561  0.0439  0.0340  0.0261  0.0198  0.0149\n",
       "[torch.FloatTensor of size 5x180]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_tensor[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\n",
       "Columns 0 to 9 \n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "\n",
       "Columns 10 to 19 \n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "\n",
       "Columns 20 to 29 \n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "\n",
       "Columns 30 to 39 \n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "\n",
       "Columns 40 to 49 \n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "\n",
       "Columns 50 to 59 \n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "\n",
       "Columns 60 to 69 \n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "\n",
       "Columns 70 to 79 \n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "\n",
       "Columns 80 to 89 \n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "\n",
       "Columns 90 to 99 \n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "\n",
       "Columns 100 to 109 \n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "\n",
       "Columns 110 to 119 \n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "\n",
       "Columns 120 to 129 \n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "\n",
       "Columns 130 to 139 \n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "\n",
       "Columns 140 to 149 \n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0003  0.0111  0.1353  0.6065\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0003  0.0111  0.1353  0.6065\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0003  0.0111  0.1353  0.6065\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0003  0.0111  0.1353  0.6065\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0003  0.0111  0.1353  0.6065\n",
       "\n",
       "Columns 150 to 159 \n",
       " 1.0000  0.6065  0.1353  0.0111  0.0003  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 1.0000  0.6065  0.1353  0.0111  0.0003  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 1.0000  0.6065  0.1353  0.0111  0.0003  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 1.0000  0.6065  0.1353  0.0111  0.0003  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 1.0000  0.6065  0.1353  0.0111  0.0003  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "\n",
       "Columns 160 to 169 \n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "\n",
       "Columns 170 to 179 \n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "[torch.FloatTensor of size 5x180]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_tensor2[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "탑 문서 수 :  1821\n",
      "u20 문서 수 :  453\n"
     ]
    }
   ],
   "source": [
    "# 갯수 맞추기\n",
    "top_indices = [i for i, x in enumerate(list_label) if x == 0]\n",
    "u20_indices = [i for i, x in enumerate(list_label) if x == 1]\n",
    "\n",
    "top_doc_size = len(top_indices)\n",
    "u20_doc_size = len(u20_indices)\n",
    "\n",
    "print('탑 문서 수 : ', top_doc_size)\n",
    "print('u20 문서 수 : ', u20_doc_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "나머지 뷰 :  torch.Size([906, 10802])\n"
     ]
    }
   ],
   "source": [
    "# 나머지 뷰 갯수 맞추기\n",
    "top_morph_list = [[morph_tensor[i], 0] for i in top_indices]\n",
    "u20_morph_list = [[morph_tensor[i], 1] for i in u20_indices]\n",
    "\n",
    "top_morph_list = random.sample(top_morph_list, u20_doc_size)\n",
    "\n",
    "morph_list = top_morph_list + u20_morph_list\n",
    "random.shuffle(morph_list)\n",
    "\n",
    "morph_data = [data[0] for data in morph_list]\n",
    "morph_data = torch.stack(morph_data, dim=0).cuda(GPU_ID)\n",
    "morph_label = [data[1] for data in morph_list]\n",
    "\n",
    "print('나머지 뷰 : ', morph_data.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "시간 뷰 :  torch.Size([906, 180])\n"
     ]
    }
   ],
   "source": [
    "# 시간 뷰 갯수 맞추기\n",
    "top_time_list = [[time_tensor[i], 0] for i in top_indices]\n",
    "u20_time_list = [[time_tensor[i], 1] for i in u20_indices]\n",
    "\n",
    "top_time_list = random.sample(top_time_list, u20_doc_size)\n",
    "\n",
    "time_list = top_time_list + u20_time_list\n",
    "random.shuffle(time_list)\n",
    "\n",
    "time_data = [data[0] for data in time_list]\n",
    "time_data = torch.stack(time_data, dim=0).cuda(GPU_ID)\n",
    "time_label = [data[1] for data in time_list]\n",
    "\n",
    "print('시간 뷰 : ', time_data.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_time_list = top_time_list + u20_time_list\n",
    "random.shuffle(cluster_time_list)\n",
    "cluster_time_data = [data[0] for data in cluster_time_list]\n",
    "cluster_time_data = torch.stack(cluster_time_data, dim=0).cuda(GPU_ID)\n",
    "cluster_time_label = [data[1] for data in cluster_time_list]\n",
    "\n",
    "with open(\"cluster_time_data.txt\", \"w\") as fp:\n",
    "    for line in cluster_time_list:\n",
    "        for element in line[0]:\n",
    "            fp.write('{:.4f}\\t'.format(element))\n",
    "        fp.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_morph_list = top_morph_list + u20_morph_list\n",
    "random.shuffle(cluster_morph_list)\n",
    "cluster_morph_data = [data[0] for data in cluster_morph_list]\n",
    "cluster_morph_data = torch.stack(cluster_morph_data, dim=0).cuda(GPU_ID)\n",
    "cluster_morph_label = [data[1] for data in cluster_morph_list]\n",
    "\n",
    "with open(\"cluster_morph_data.txt\", \"w\") as fp:\n",
    "    for line in cluster_morph_list:\n",
    "        for element in line[0]:\n",
    "            fp.write('{:.4f}\\t'.format(element))\n",
    "        fp.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SimpleAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, int(input_size/2)),\n",
    "            nn.Linear(int(input_size/2), int(input_size/4)))\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(int(input_size/4), int(input_size/2)),\n",
    "            nn.Linear(int(input_size/2), input_size), nn.ReLU())  # ReLU, Tanh\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    def getRepresentation(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_DATA_NUM = u20_doc_size*2\n",
    "TRAIN_DATA_NUM = int(TOTAL_DATA_NUM*0.8)\n",
    "TEST_DATA_NUM = TOTAL_DATA_NUM - TRAIN_DATA_NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBatch(data, batch_size=128, isTrain=True):\n",
    "    if isTrain:\n",
    "        startIdx = 0\n",
    "        endIdx = TRAIN_DATA_NUM\n",
    "    else:\n",
    "        startIdx = TRAIN_DATA_NUM\n",
    "        endIdx = len(data)\n",
    "        \n",
    "    for idx in range(startIdx, endIdx, batch_size):\n",
    "        batch = data[idx:min(idx + batch_size, endIdx)]\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def trainModel(model, inputs, num_epoch=200, batch_size=16, lr=1e-3, w_decay=1e-5, isPrintEval=False, isPrint=True, isPlot=True,\n",
    "               print_every=5, plot_every=5):\n",
    "    BATCH_SIZE = batch_size\n",
    "    NUM_EPOCH = num_epoch\n",
    "    \n",
    "    LR = 1e-3\n",
    "    WEIGHT_DECAY = 1e-5\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    \n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "    \n",
    "    for epoch in tqdm(range(1, NUM_EPOCH+1)):\n",
    "        for batch in getBatch(inputs, batch_size=BATCH_SIZE):\n",
    "            batch = Variable(batch).cuda(GPU_ID)\n",
    "            # ===================forward=====================\n",
    "            output = model(batch).cuda(GPU_ID)\n",
    "            loss = criterion(output, batch)\n",
    "            # ===================backward====================\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            print_loss_total += loss.data[0]\n",
    "            plot_loss_total += loss.data[0]\n",
    "            \n",
    "        # ===================log========================\n",
    "        if epoch % print_every == 0 and isPrint:\n",
    "            print_loss_avg = print_loss_total / (print_every*batch_size)\n",
    "            print_loss_total = 0\n",
    "            print_eval_loss_avg = -1\n",
    "            if isPrintEval:\n",
    "                print_eval_loss_avg = evalModel(model, inputs)[0].data.tolist()[0]/TEST_DATA_NUM\n",
    "            print('epoch [{}/{}], loss:{:.5f}, eval_loss:{:.5f}'.format(epoch, NUM_EPOCH, print_loss_avg, print_eval_loss_avg))\n",
    "        \n",
    "        if epoch % plot_every == 0 and isPlot:\n",
    "            plot_loss_avg = plot_loss_total / (print_every*batch_size)\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "    \n",
    "    if isPlot:\n",
    "        showPlot(plot_losses)\n",
    "    #torch.save(model.state_dict(), './sim_autoencoder.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalModel(model, inputs):\n",
    "    BATCH_SIZE = 256\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    loss = 0\n",
    "    result = []\n",
    "    for batch in getBatch(inputs, batch_size=BATCH_SIZE, isTrain=False):\n",
    "        batch = Variable(torch.FloatTensor(batch.tolist())).cuda(GPU_ID)\n",
    "        \n",
    "        output = model(batch).cuda(GPU_ID)\n",
    "        result.append(output)\n",
    "        loss += criterion(output, batch)[0]\n",
    "        \n",
    "    return loss, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch_list = [300]\n",
    "n_test = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:37<00:00,  7.90it/s]\n"
     ]
    }
   ],
   "source": [
    "epoch_acc_list = []\n",
    "epoch_acc_avg_list = []\n",
    "\n",
    "for n_epoch in n_epoch_list:\n",
    "    \n",
    "    acc_list = []\n",
    "    for t in range(n_test):\n",
    "        # 시간뷰 (180 dim)\n",
    "        # Training\n",
    "        inputs = time_data\n",
    "        INPUT_SIZE = inputs.size()[1]\n",
    "        \n",
    "        model_time = SimpleAutoencoder(INPUT_SIZE).cuda(GPU_ID)\n",
    "        trainModel(model_time, inputs, num_epoch=n_epoch, print_every=102, plot_every=102, isPrint=False, isPlot=False)\n",
    "        \n",
    "        # Clustering\n",
    "        inputs = Variable(cluster_time_data).cuda(GPU_ID)\n",
    "        batch_rep = model_time.getRepresentation(inputs).data.tolist()\n",
    "        \n",
    "        kmeans_time = KMeans(n_clusters=2, random_state=0).fit(batch_rep)\n",
    "        gen_label_time = kmeans_time.labels_\n",
    "        \n",
    "        correct_cnt_morph = max([sum([1 for tar, gen in zip(cluster_time_label, gen_label_time) if tar == gen]),\n",
    "                                 sum([1 for tar, gen in zip(cluster_time_label, gen_label_time) if tar != gen])])\n",
    "        acc_morph = correct_cnt_morph / TOTAL_DATA_NUM\n",
    "        acc_list.append(acc_morph)\n",
    "    \n",
    "    epoch_acc_list.append(acc_list)\n",
    "    epoch_acc_avg_list.append(sum(acc_list) / n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10, 0.5960264900662252), (20, 0.5540838852097131), (30, 0.8642384105960265)]"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(n_epoch_list, epoch_acc_avg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(300, 0.8520971302428256)]"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(n_epoch_list, epoch_acc_avg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 10.17it/s]\n"
     ]
    }
   ],
   "source": [
    "inputs = time_data\n",
    "INPUT_SIZE = inputs.size()[1]\n",
    "\n",
    "model_time = SimpleAutoencoder(INPUT_SIZE).cuda(GPU_ID)\n",
    "trainModel(model_time, inputs, num_epoch=n_epoch, print_every=102, plot_every=102, isPrint=False, isPlot=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "\n",
       "Columns 0 to 9 \n",
       " 0.4163  0.1423  0.2675 -0.9332 -0.7013  0.0287 -0.3476  0.5854  0.6664  0.0532\n",
       "-0.3041 -0.3994 -0.5525 -0.2535 -0.6925  0.4107 -0.1130  0.5156  0.2931  0.0101\n",
       "-0.3041 -0.3994 -0.5525 -0.2535 -0.6925  0.4107 -0.1130  0.5156  0.2931  0.0101\n",
       " 0.0143 -0.2531 -0.2548 -0.6115 -0.8052  0.2831 -0.2114  0.5732  0.4245 -0.0146\n",
       "-0.3041 -0.3994 -0.5525 -0.2535 -0.6925  0.4107 -0.1130  0.5156  0.2931  0.0101\n",
       "-0.3606 -0.4095 -0.5999 -0.1775 -0.6487  0.4291 -0.0919  0.4979  0.2737  0.0245\n",
       " 0.4163  0.1423  0.2675 -0.9332 -0.7013  0.0287 -0.3476  0.5854  0.6664  0.0532\n",
       " 0.2835  0.8841  0.9959 -0.2389 -0.1301 -0.5372 -0.6970  0.3201  0.6392  0.1369\n",
       " 0.5513  0.5115  0.6909 -0.8942 -0.4811 -0.2226 -0.4836  0.5427  0.8060  0.1349\n",
       " 0.6090 -0.0102 -0.0413 -0.8638  0.1173 -1.1606  0.3989 -0.1331 -0.4039 -0.3913\n",
       "\n",
       "Columns 10 to 19 \n",
       " 0.8163  0.0946  0.2205  0.5516 -0.4612 -0.4865  0.4240  0.9357  0.7644  0.6424\n",
       " 0.9595 -0.2600 -0.4276  0.3069  0.1719 -0.6785  0.4339  0.3427  0.3451  0.5559\n",
       " 0.9595 -0.2600 -0.4276  0.3069  0.1719 -0.6785  0.4339  0.3427  0.3451  0.5559\n",
       " 1.0322 -0.0597 -0.1038  0.4944 -0.0434 -0.6727  0.4856  0.6700  0.5617  0.6296\n",
       " 0.9595 -0.2600 -0.4276  0.3069  0.1719 -0.6785  0.4339  0.3427  0.3451  0.5559\n",
       " 0.9240 -0.3043 -0.4982  0.2552  0.2015 -0.6660  0.4172  0.2687  0.2956  0.5332\n",
       " 0.8163  0.0946  0.2205  0.5516 -0.4612 -0.4865  0.4240  0.9357  0.7644  0.6424\n",
       "-0.2123 -0.1052  0.2911  0.0028 -1.1587  0.1369 -0.0800  0.6771  0.6662  0.3712\n",
       " 0.4113  0.0731  0.3408  0.4225 -0.8098 -0.2334  0.2459  0.9457  0.8215  0.5922\n",
       " 0.4430  0.1843  0.0027 -1.5284 -0.2795 -0.9739  0.1343 -0.4533 -0.9637 -0.6701\n",
       "\n",
       "Columns 20 to 29 \n",
       " 0.7520 -0.7546 -0.4790  0.4646  0.7069  0.1269  0.1212  0.0022 -0.2192 -0.4528\n",
       " 0.4415 -0.3751 -0.0083  0.5431  0.5960  0.0469 -0.1095 -0.6473 -0.2412  0.8020\n",
       " 0.4415 -0.3751 -0.0083  0.5431  0.5960  0.0469 -0.1095 -0.6473 -0.2412  0.8020\n",
       " 0.5866 -0.5623 -0.3106  0.6460  0.7609  0.0889 -0.0914 -0.4379 -0.2438  0.3763\n",
       " 0.4415 -0.3751 -0.0083  0.5431  0.5960  0.0469 -0.1095 -0.6473 -0.2412  0.8020\n",
       " 0.4099 -0.3369  0.0638  0.4968  0.5433  0.0366 -0.1003 -0.6706 -0.2385  0.8578\n",
       " 0.7520 -0.7546 -0.4790  0.4646  0.7069  0.1269  0.1212  0.0022 -0.2192 -0.4528\n",
       " 0.8831 -0.3329  0.1329 -0.4553 -0.1233  0.1567  0.5848  0.5902 -0.0493 -1.3696\n",
       " 0.8518 -0.7569 -0.3493  0.1199  0.4546  0.1467  0.3932  0.3436 -0.1707 -1.0722\n",
       "-0.4683  0.4522 -0.3389 -0.2387  0.3161  0.2222  0.3342  1.0717  0.8666  0.7401\n",
       "\n",
       "Columns 30 to 39 \n",
       "-0.1592  0.1730  0.5691  0.3840 -0.6820  0.1386  0.3584  0.7524 -0.2376 -0.3716\n",
       "-0.6895 -0.2123  0.5141  0.4116 -0.1583  0.0311  0.3191  0.7667  0.3842 -0.6237\n",
       "-0.6895 -0.2123  0.5141  0.4116 -0.1583  0.0311  0.3191  0.7667  0.3842 -0.6237\n",
       "-0.5437 -0.0255  0.6075  0.4330 -0.3859  0.0662  0.4216  0.8920  0.1973 -0.5409\n",
       "-0.6895 -0.2123  0.5141  0.4116 -0.1583  0.0311  0.3191  0.7667  0.3842 -0.6237\n",
       "-0.7026 -0.2521  0.4854  0.4017 -0.1196  0.0277  0.2843  0.7221  0.4064 -0.6370\n",
       "-0.1592  0.1730  0.5691  0.3840 -0.6820  0.1386  0.3584  0.7524 -0.2376 -0.3716\n",
       " 0.9638  0.2705 -0.1079 -0.0556 -0.5202  0.3828 -0.1784 -0.5180 -1.1406  0.1257\n",
       " 0.2676  0.2639  0.3748  0.2577 -0.7630  0.2157  0.1640  0.3502 -0.6701 -0.1860\n",
       " 0.4426 -0.4332  0.4698 -1.3529 -0.4318  0.2807 -0.5993 -0.5268  0.3185  0.5821\n",
       "\n",
       "Columns 40 to 44 \n",
       "-0.5675  0.3722  0.2891 -0.6287  0.2279\n",
       "-0.3472  0.2406  0.0339  0.5365 -0.3854\n",
       "-0.3472  0.2406  0.0339  0.5365 -0.3854\n",
       "-0.5084  0.2714  0.1661  0.0379 -0.1275\n",
       "-0.3472  0.2406  0.0339  0.5365 -0.3854\n",
       "-0.3053  0.2427  0.0090  0.6258 -0.4331\n",
       "-0.5675  0.3722  0.2891 -0.6287  0.2279\n",
       "-0.2187  0.3116  0.0360 -0.9477  0.5803\n",
       "-0.4802  0.4202  0.2587 -0.9759  0.4459\n",
       " 0.7831 -0.2893  0.6647 -0.4823 -0.2211\n",
       "[torch.cuda.FloatTensor of size 10x45 (GPU 2)]"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clustering\n",
    "inputs = Variable(cluster_time_data).cuda(GPU_ID)\n",
    "batch_rep = model_time.getRepresentation(inputs)\n",
    "\n",
    "batch_rep[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 0, 0, 0, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_time_label[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_time = KMeans(n_clusters=2, random_state=0).fit(batch_rep)\n",
    "gen_label_time = kmeans_time.labels_\n",
    "\n",
    "correct_cnt_morph = max([sum([1 for tar, gen in zip(cluster_time_label, gen_label_time) if tar == gen]),\n",
    "                         sum([1 for tar, gen in zip(cluster_time_label, gen_label_time) if tar != gen])])\n",
    "acc_morph = correct_cnt_morph / TOTAL_DATA_NUM\n",
    "acc_list.append(acc_morph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "1 1\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "1 1\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "1 1\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "1 1\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "1 1\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "1 1\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "1 1\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "0 0\n",
      "1 1\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "0 0\n",
      "1 1\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "1 1\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "1 1\n",
      "1 1\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "1 1\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "1 1\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "0 0\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "0 0\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "0 0\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "1 1\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "1 1\n",
      "0 0\n",
      "1 1\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "1 1\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "0 0\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "1 1\n",
      "1 0\n",
      "0 0\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "1 1\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "1 1\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "1 1\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "1 1\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "1 1\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "1 0\n",
      "0 0\n",
      "1 1\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "0 0\n",
      "1 1\n",
      "1 1\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "1 1\n",
      "1 1\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "0 0\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "0 0\n",
      "1 1\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "1 1\n",
      "1 1\n",
      "0 0\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "0 0\n",
      "1 1\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "1 1\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "1 0\n"
     ]
    }
   ],
   "source": [
    "for tar, gen in zip(cluster_time_label, gen_label_time):\n",
    "    print(tar, gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6/50 [00:01<00:08,  4.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [5/50], loss:0.00149, eval_loss:-1.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [00:02<00:07,  5.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [10/50], loss:0.00060, eval_loss:-1.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 16/50 [00:03<00:06,  5.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [15/50], loss:0.00060, eval_loss:-1.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 21/50 [00:04<00:05,  5.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [20/50], loss:0.00060, eval_loss:-1.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 26/50 [00:05<00:04,  5.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [25/50], loss:1.98049, eval_loss:-1.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 31/50 [00:06<00:03,  5.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [30/50], loss:0.01275, eval_loss:-1.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 36/50 [00:07<00:02,  5.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [35/50], loss:0.00060, eval_loss:-1.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 41/50 [00:08<00:01,  5.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [40/50], loss:0.00060, eval_loss:-1.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 46/50 [00:09<00:00,  5.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [45/50], loss:0.00060, eval_loss:-1.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:09<00:00,  5.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [50/50], loss:0.00060, eval_loss:-1.00000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 나머지뷰 (10802 dim)\n",
    "inputs = morph_data\n",
    "INPUT_SIZE = inputs.size()[1]\n",
    "\n",
    "model_morph = SimpleAutoencoder(INPUT_SIZE).cuda(GPU_ID)\n",
    "trainModel(model_morph, inputs, num_epoch=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-295-53fa299e7b36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster_morph_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGPU_ID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mbatch_rep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_morph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetRepresentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/pytorch-py35/lib/python3.5/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mtolist\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msubt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msubt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/pytorch-py35/lib/python3.5/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msubt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msubt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/pytorch-py35/lib/python3.5/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mtolist\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msubt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msubt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/pytorch-py35/lib/python3.5/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msubt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msubt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/pytorch-py35/lib/python3.5/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(i)\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnelement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_range\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 나머지 뷰\n",
    "# Train\n",
    "inputs = Variable(cluster_morph_data).cuda(GPU_ID)\n",
    "batch_rep = model_morph.getRepresentation(inputs).data.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_morph = KMeans(n_clusters=2, random_state=0).fit(batch_rep)\n",
    "gen_label_morph = kmeans_morph.labels_\n",
    "\n",
    "correct_cnt_morph = max(gen_label_morph[u20_doc_size:].tolist().count(0) + gen_label_morph[:u20_doc_size].tolist().count(1),\n",
    "                        gen_label_morph[u20_doc_size:].tolist().count(1) + gen_label_morph[:u20_doc_size].tolist().count(0))\n",
    "acc_morph = correct_cnt_morph / TOTAL_DATA_NUM\n",
    "acc_morph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gen_label_morph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
